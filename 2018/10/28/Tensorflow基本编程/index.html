<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Tensorflow," />










<meta name="description" content="Tensorflow基本编程">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow基本编程">
<meta property="og:url" content="http://yoursite.com/2018/10/28/Tensorflow基本编程/index.html">
<meta property="og:site_name" content="JinZeng&#39;s Blog">
<meta property="og:description" content="Tensorflow基本编程">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/10/28/Tensorflow基本编程/tensor张量.png">
<meta property="og:updated_time" content="2019-01-17T12:32:54.712Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow基本编程">
<meta name="twitter:description" content="Tensorflow基本编程">
<meta name="twitter:image" content="http://yoursite.com/2018/10/28/Tensorflow基本编程/tensor张量.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/28/Tensorflow基本编程/"/>





  <title>Tensorflow基本编程 | JinZeng's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JinZeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">nlp</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/28/Tensorflow基本编程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JinZeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JinZeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Tensorflow基本编程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-28T10:38:50+08:00">
                2018-10-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
              <div class="post-description">
                  Tensorflow基本编程
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Tensorflow基本编程"><a href="#Tensorflow基本编程" class="headerlink" title="Tensorflow基本编程"></a><center>Tensorflow基本编程</center></h1><ul>
<li>张量的阶和形状是不同的概念。一阶张量表示数组，二阶张量表示矩阵。但形状指的是二阶张量的具体形状，比如[3，4]。形状中的None值比如[None, 784]，这是一个二阶张量，None表示该维度可以是任何长度。</li>
</ul>
<p><img src="/2018/10/28/Tensorflow基本编程/tensor张量.png" alt="tensor张量"></p>
<p>tensor.get_shape().as_list()</p>
<p>xxx.get_shape()中的xxx的数据类型必须是tensor,且返回的是一个tuple.可以通过xxx.get_shape().as_list()得到一个list。</p>
<p>tf.shape(xxx)</p>
<p>tf.shape(xxx)中xxx数据的类型可以是tensor,list,array</p>
<p>numpy数组的shape获取：</p>
<p>np.shape(数组名)</p>
<p>变量的静态形状与动态形状：</p>
<p>静态（推测）形状：创建一个张量或者由操作推导出一个张量时，初始状态的形状</p>
<p>​    tf.Tensor.get_shape:获取静态形状</p>
<p>动态（真实）形状：描述原始张量在执行过程中的一种形状</p>
<p>​    tf.shape(tf.Tensor):如果在运行的时候想知道None到底是多少,只能通过tf.shape(tensor)[0]这种方式来获得</p>
<p>​    tf.reshape:创建一个具有不同动态形状的新张量</p>
<p>​    </p>
<h2 id="Tensorflow数据读取"><a href="#Tensorflow数据读取" class="headerlink" title="Tensorflow数据读取"></a>Tensorflow数据读取</h2><ul>
<li><p>feeding供给数据：<strong>通过run()函数和eval()函数输入feed_dict参数，可以启动运算过程。设计placeholder节点的唯一目的就是为了提供数据供给feeding的方法</strong>。placeholder节点被声明的时候是未被初始化的，不会包含任何数据。如果运行时没有给它供给数据，Tensorflow运行的时候就会报错。</p>
</li>
<li><p>文件读取管线，其相关步骤如下所示：</p>
<ul>
<li><p>创建文件名列表</p>
</li>
<li><p>选择是否配置文件名乱序shuffing</p>
</li>
<li><p>选择配置最大迭代次数epoch_limit</p>
</li>
<li><p>创建文件名队列：将文件名列表交给tf.train.string_input_producer()函数，会生成一个先入先出的队列，<strong>对应的文件阅读器会需要它来阅读数据</strong>。有一个QueueRunner的工作线程专门负责这一生成文件名队列的过程。</p>
<p>QueueRunner的工作线程是独立于文件阅读器的线程，因此乱序和将文件名推入队列这些过程不会阻塞文件阅读器的运行。<strong>也就是QueueRunner的运行不会阻塞文件阅读器的运行</strong>。</p>
</li>
<li><p>创建针对文件格式的阅读器</p>
</li>
<li><p>记录解析器</p>
</li>
<li><p>可配置的预处理器</p>
</li>
<li><p>样本队列</p>
</li>
</ul>
<p><strong>注意：在调用run函数或者eval函数之前，必须用tf.train.start_queue_runners来将文件名填充到队列。否则，read方法操作将会被阻塞到文件名队列中有值位置。</strong></p>
<p>所以，一个多线程的数据读取往往需要我们创建两个队列，<strong>一个是文件名队列string_input_producer，另一个队列tf.train.shuffle_batch()来执行输入样本的训练，评价推理准备</strong></p>
</li>
<li><p>从<strong>二进制文件</strong>读取固定长度的记录，可以使用<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#FixedLengthRecordReader" target="_blank" rel="noopener"><code>tf.FixedLengthRecordReader</code></a>的<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#decode_raw" target="_blank" rel="noopener"><code>tf.decode_raw</code></a>操作。decode_raw操作可以将一个字符串转换为一个uint的张量</p>
</li>
<li><p>Coordinator：可以用来同时停止多个工作线程并且向那个在等待所有工作线程的程序报告异常。包括方法有should_stop()，request_stop()，join()等。QueueRunner用来协调多个工作线程同时将多个张量推入同意队列中。</p>
</li>
<li><p>QueueRunner会创建一组线程，这些线程可以重复的执行入队操作。这些线程使用同一个Coordinator来处理线程同步终止。通常在训练程序中，会创建一个QueueRunner来运行几个线程，这几个线程处理样本，并将样本推入队列。创建一个Coordinator，让queueRunner来启动这些线程。创建一个训练的循环，并且使用Coordinator来控制QueueRunner的线程的终止。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create a queue runner that will run 4 threads in parallel to enqueue</span><br><span class="line"># examples.</span><br><span class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"># Launch the graph.</span><br><span class="line">sess = tf.Session()</span><br><span class="line"># Create a coordinator, launch the queue runner threads.</span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=True)</span><br><span class="line"># Run the training loop, controlling termination with the coordinator.</span><br><span class="line"><span class="function"><span class="keyword">for</span> step in <span class="title">xrange</span><span class="params">(<span class="number">1000000</span>)</span>:</span></span><br><span class="line"><span class="function">    <span class="keyword">if</span> coord.<span class="title">should_stop</span><span class="params">()</span>:</span></span><br><span class="line"><span class="function">        <span class="keyword">break</span></span></span><br><span class="line"><span class="function">    sess.<span class="title">run</span><span class="params">(train_op)</span></span></span><br><span class="line"><span class="function"># When done, ask the threads to stop.</span></span><br><span class="line"><span class="function">coord.<span class="title">request_stop</span><span class="params">()</span></span></span><br><span class="line"><span class="function"># And wait <span class="keyword">for</span> them to actually <span class="keyword">do</span> it.注意join函数是等待被指定的线程终止</span></span><br><span class="line"><span class="function">coord.<span class="title">join</span><span class="params">(threads)</span></span></span><br></pre></td></tr></table></figure>
<p>这里有一个问题是：为什么request_stop会在join函数前面。</p>
</li>
</ul>
<p>不要重新引导主机</p>
<h2 id="tf-train-slice-input-producer、tf-train-batch、tf-train-shuffle-batch、tf-train-shuffle-batch-join"><a href="#tf-train-slice-input-producer、tf-train-batch、tf-train-shuffle-batch、tf-train-shuffle-batch-join" class="headerlink" title="tf.train.slice_input_producer、tf.train.batch、tf.train.shuffle_batch、tf.train.shuffle_batch_join"></a>tf.train.slice_input_producer、tf.train.batch、tf.train.shuffle_batch、tf.train.shuffle_batch_join</h2><ul>
<li><p>tf.train.slice_input_producer解析</p>
<p>是一个tensor生成器，该函数接受一个tensor列表，然后根据该列表返回一个生成器。<strong>按照设定，每次从一个tensor列表中按照顺序或者随机抽取出一个tensor放入列表（注意是一个）</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slice_input_producer(tensor_list, num_epochs=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>, seed=<span class="keyword">None</span>,</span><br><span class="line">                         capacity=<span class="number">32</span>, shared_name=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>其中的第一个参数tensor_list：包含一系列tensor的列表，表中的第一维度的值必须相等，即个数值必须相等。如训练样本X和label Y的第一维度值相等。</p>
<p>​</p>
<h2 id="dynamic-rnn-函数"><a href="#dynamic-rnn-函数" class="headerlink" title="dynamic_rnn( )函数"></a>dynamic_rnn( )函数</h2><p>该函数至少需要输入两个数据，一个是<strong>padding后的数据，一个是sequence_length记录句子实际长度</strong>。再提醒一下，即使用dynamic_rnn( )也需要输入padding过后的数据。返回值如下：</p>
<p>每个time_step的输出值[batch_size，num_step，hidden_size]，以及最后一个时间步的状态值（是一个元组(c，h)）其中c与h的维度均为[batch_size，hidden_size]</p>
<p>dynamic_rnn()实现的功能可以让<strong>不同迭代</strong>传入的batch可以是不同长度的数据，但同一迭代一个batch内的数据长度必须是相同的。</p>
</li>
</ul>
<h2 id="不同的计算图Graph如何实现参数共享"><a href="#不同的计算图Graph如何实现参数共享" class="headerlink" title="不同的计算图Graph如何实现参数共享"></a>不同的计算图Graph如何实现参数共享</h2><h2 id="会话和计算图的理解"><a href="#会话和计算图的理解" class="headerlink" title="会话和计算图的理解"></a>会话和计算图的理解</h2><ul>
<li>Tensorflow的程序分为两个阶段，一个是图的构建阶段，在Graph中设置。一个是会话阶段，实际执行训练op的阶段。</li>
<li>没有显式声明图，所有操作都是在默认计算图中</li>
<li>​</li>
</ul>
<h2 id="Tensor和numpy数组的转换"><a href="#Tensor和numpy数组的转换" class="headerlink" title="Tensor和numpy数组的转换"></a>Tensor和numpy数组的转换</h2><p>tensor转为numpy数组tensor.eval函数</p>
<p>numpy数组转tensor：tf.convert_to_tensor(numpy数组)</p>
<h2 id="Tensor和变量的区别"><a href="#Tensor和变量的区别" class="headerlink" title="Tensor和变量的区别"></a>Tensor和变量的区别</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><a href="https://segmentfault.com/a/1190000008793389" target="_blank" rel="noopener">https://segmentfault.com/a/1190000008793389</a></p>
<p><a href="https://www.bbsmax.com/A/8Bz8XnaVzx/" target="_blank" rel="noopener">https://www.bbsmax.com/A/8Bz8XnaVzx/</a></p>
<ul>
<li><p>tensor用来表示所有的数据，计算图中，操作间传递的数据都是tensor。</p>
</li>
<li><p><strong>如果要获取tensor的实际值，必须要在会话中获取，即需要传递一个session的参数，只在计算图中是无法获取值的</strong></p>
</li>
<li><p>tf.cast( )函数，用在Session之外，是一个op操作</p>
</li>
<li><p>tf.to_int32( )</p>
</li>
<li><p>tensor拼接</p>
<ul>
<li><p><code>tf.concat(values, axis, name=&#39;concat&#39;)</code>：按照指定的<strong>已经存在</strong>的轴进行拼接</p>
</li>
<li><p><code>tf.stack(values, axis=0, name=&#39;stack&#39;)</code>：按照指定的<strong>新建</strong>的轴进行拼接</p>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[1, 2, 3], [4, 5, 6]]</span><br><span class="line">t2 = [[7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat([t1, t2], 0) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat([t1, t2], 1) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span><br><span class="line">tf.stack([t1, t2], 0)  ==&gt; [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]</span><br><span class="line">tf.stack([t1, t2], 1)  ==&gt; [[[1, 2, 3], [7, 8, 9]], [[4, 5, 6], [10, 11, 12]]]</span><br><span class="line">tf.stack([t1, t2], 2)  ==&gt; [[[1, 7], [2, 8], [3, 9]], [[4, 10], [5, 11], [6, 12]]]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>tensor抽取：</p>
<ul>
<li><code>tf.slice(input_, begin, size, name=None)</code>：按照指定的下标范围抽取<strong>连续</strong>区域的子集</li>
<li><code>tf.gather(params, indices, validate_indices=None, name=None)</code>：按照指定的下标集合从<code>axis=0</code>中抽取子集，适合抽取<strong>不连续</strong>区域的子集</li>
</ul>
</li>
<li><p>tf.concat(values,dim,name=’concat’)</p>
<p>在哪个维度拼接，就在哪个维度求和。维度为(3,1)和（3,1）的两个向量，在第一维上连接，就是(6,1);在第二维连接，就是(3,2)</p>
</li>
<li><p>独热编码</p>
<p><a href="https://blog.csdn.net/qq_22812319/article/details/83374125" target="_blank" rel="noopener">https://blog.csdn.net/qq_22812319/article/details/83374125</a></p>
<p><a href="https://blog.csdn.net/ghy_111/article/details/80362597" target="_blank" rel="noopener">https://blog.csdn.net/ghy_111/article/details/80362597</a></p>
<p>tf_onehot()无法对多标记进行编码，因为当你输入一个矩阵为indices时，系统会判定你输入的是一个batch的数据</p>
</li>
<li><p>多线程读取数据的时候，tensor list的第一个维度必须相等</p>
<p>tensor shape为( )表示一个标量，为(？, )表示一个向量，这个向量长度不定</p>
</li>
</ul>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><ul>
<li>变量包含张量（tensor）存放在内存的缓存区。变量需要初始化，模型训练后它们必须存储到磁盘。</li>
<li>当创建一个变量的时候，可以将一个<strong>张量（哈哈，注意张量或者Tensor）</strong>作为初始值传入构造函数Variable。需要指定<strong>张量的shape</strong>，这个形状会自动变为变量的shape</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</span><br><span class="line">                      name=<span class="string">"weights"</span>)</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</span><br></pre></td></tr></table></figure>
<p>创建weights和biases都有一个初始的Tensor张量值。这实际上是一个tf.assign的过程，给变量赋实际tensor值。</p>
<p>但是不要以为这样就行了，还需要在session执行一个变量初始化的操作才可以！！！！变量的初始化必须在模型的其它操作运行之前先明确地完成。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Create two variables.</span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</span><br><span class="line">                      name=<span class="string">"weights"</span>)</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</span><br><span class="line">...</span><br><span class="line"># Add an op to initialize the variables.</span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"># Later, when launching the model</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  # Run the init operation.</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  ...</span><br><span class="line">  # Use the model</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<ul>
<li>变量：维护图执行过程中的状态信息，比如模型参数可以用变量来表示。模型参数是状态的一种表示。<strong>通常会将一个模型中的参数表示为一组变量。</strong>例如，可以将神经网络的权重作为某个变量存储在一个tensor中。<ul>
<li>变量用于存储网络中的权重矩阵等变量，而Tensor更多的是中间结果</li>
<li>Variable变量是会显示分配内存空间的，需要初始化操作，由Session管理，可以进行存储、读取、更改等操作。而Const，Zeros创建的Tensor，是记录在Graph中，没有单独的内存空间。</li>
<li>Tensor可以使用的地方，几乎都可以使用Variable</li>
</ul>
</li>
<li>变量的存储与回复<ul>
<li>如果需要保存和恢复模型变量的不同子集，可以创建任意多个saver对象。同一个变量可以被列入多个saver对象中，只有当saver的restore( )函数运行时，对应变量的值才会改变。</li>
</ul>
</li>
</ul>
<h3 id="变量共享"><a href="#变量共享" class="headerlink" title="变量共享"></a>变量共享</h3><p><a href="http://www.tensorfly.cn/tfdoc/how_tos/variable_scope.html" target="_blank" rel="noopener">http://www.tensorfly.cn/tfdoc/how_tos/variable_scope.html</a></p>
<ul>
<li>tf.variable()无法做到变量共享</li>
</ul>
<p>定义了一个卷积函数conv_relu（相关参数为weights和bises）。假设我们的卷积神经网络有两层，conv1和conv2，这两层的weights和biases应该是不同的。那么可以用命名域：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line"><span class="function">    with tf.<span class="title">variable_scope</span><span class="params">(<span class="string">"conv1"</span>)</span>:</span></span><br><span class="line"><span class="function">        # Variables created here will be named "conv1/weights", "conv1/biases".</span></span><br><span class="line"><span class="function">        relu1 </span>= conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br><span class="line">    with tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        # Variables created here will be named "conv2/weights", "conv2/biases".</span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br></pre></td></tr></table></figure>
<p>假设我们现在要用my_image_filter跑两张图片：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result1 = my_image_filter(image1)</span><br><span class="line">result2 = my_image_filter(image2)</span><br><span class="line"># Raises ValueError(... conv1/weights already exists ...)</span><br></pre></td></tr></table></figure>
<p>会报错！如果你想共享他们，你需要像下面使用的一样，通过<code>reuse_variables()</code>这个方法来指定.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&quot;image_filters&quot;) as scope:</span><br><span class="line">    result1 = my_image_filter(image1)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    result2 = my_image_filter(image2)</span><br></pre></td></tr></table></figure>
<ul>
<li>tf.variable_scope()</li>
</ul>
<p>获取当前变量的作用域：tf.get_variable_scope()进行检索，通过调用<code>tf.get_variable_scope().reuse_variables()</code>设置为<code>True</code> .</p>
<p>获取变量作用域并加以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&quot;foo&quot;) as foo_scope:</span><br><span class="line">    v = tf.get_variable(&quot;v&quot;, [1])</span><br><span class="line">with tf.variable_scope(foo_scope)</span><br><span class="line">    w = tf.get_variable(&quot;w&quot;, [1])</span><br><span class="line">with tf.variable_scope(foo_scope, reuse=True)</span><br><span class="line">    v1 = tf.get_variable(&quot;v&quot;, [1])</span><br><span class="line">    w1 = tf.get_variable(&quot;w&quot;, [1])</span><br><span class="line">assert v1 == v</span><br><span class="line">assert w1 == w</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/28/python生成器/" rel="next" title="python生成器">
                <i class="fa fa-chevron-left"></i> python生成器
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/29/Linux操作/" rel="prev" title="Linux操作">
                Linux操作 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JinZeng</p>
              <p class="site-description motion-element" itemprop="description">nlper</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/jin-ge-ge-21-50-31/activities" target="_blank" title="zhihu">
                      
                        <i class="fa fa-fw fa-globe"></i>zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow基本编程"><span class="nav-number">1.</span> <span class="nav-text">Tensorflow基本编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow数据读取"><span class="nav-number">1.1.</span> <span class="nav-text">Tensorflow数据读取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-train-slice-input-producer、tf-train-batch、tf-train-shuffle-batch、tf-train-shuffle-batch-join"><span class="nav-number">1.2.</span> <span class="nav-text">tf.train.slice_input_producer、tf.train.batch、tf.train.shuffle_batch、tf.train.shuffle_batch_join</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dynamic-rnn-函数"><span class="nav-number">1.3.</span> <span class="nav-text">dynamic_rnn( )函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同的计算图Graph如何实现参数共享"><span class="nav-number">1.4.</span> <span class="nav-text">不同的计算图Graph如何实现参数共享</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#会话和计算图的理解"><span class="nav-number">1.5.</span> <span class="nav-text">会话和计算图的理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor和numpy数组的转换"><span class="nav-number">1.6.</span> <span class="nav-text">Tensor和numpy数组的转换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor和变量的区别"><span class="nav-number">1.7.</span> <span class="nav-text">Tensor和变量的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">1.7.1.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量"><span class="nav-number">1.7.2.</span> <span class="nav-text">变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量共享"><span class="nav-number">1.7.3.</span> <span class="nav-text">变量共享</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JinZeng</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
