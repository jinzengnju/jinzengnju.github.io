<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="深度学习," />










<meta name="description" content="seq2seq与Attention机制">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="seq2seq与Attention机制">
<meta property="og:url" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/index.html">
<meta property="og:site_name" content="JinZeng&#39;s Blog">
<meta property="og:description" content="seq2seq与Attention机制">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/3c26dc6175b4e85196d0204a0ff0a339.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/1def1af24886828c9e28cdd0f491f94e.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/8015d70afd6fd139d698cfcce1e204f2.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/08b30a3eed06f21457d32031fe66649e.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/454fac3258597c811778954f9f2d2ab2.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/55f3494c4127aa08148dcb273063cc26.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/a4958aeeb7ae5b0a277111935b161be0.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/bcbfe42587a078103e8aaf9c9d7bd870.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/3deca48973c2383bfe7521c62dda3484.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/5aea7c937fc04da09b56311646f75015.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/66e9e3b23a0239690eeda744a8cf5147.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/5fb8b74dc601441211fa0ea4a72090f6.jpg">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/5c3653e5adc717a4d74bcc6903535a15.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/1e700a0d1d359df36e69b1c292da0c6f.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/75450dcdbdbbb2bbb966f5e3feb74551.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/cb72243a207c961e4a0e63fcca414627.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/bb38fac0cacf1432540bd1854d15adb2.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/18eebbbb56bc96ac5a50f74579b598a1.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/6ee2e9675718c6fb50dac37231871d8f.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/a5751311d8086511c1a5d2da996f79da.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/710418ff7e7a7abdb1cbc6ef95ce9178.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/1b2ec6741b499ce8bb46097e0300f5fa.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/90bf091dbdc36e3b08d92e4de3dea8b7.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/6a3345421be62897b556458a353c3d25.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/0284f856a5de915ad115471293704447.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/000bb0b82a1cfa56c558e93cf826c347.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/0744aaf83d952aaab55d47d721cd6367.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/1e3f37d7e85992cb12ce0b40cdcb5fbc.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/bce67264ddda775bef14e6f3154e7a99.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/a2bfd2b8314d5eda5150e70649973e96.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/b284b79814abd3dd08f460e9266eeaf3.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/510dda99021713b52c1604621f1d213c.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/191a3008d1a58ea913c19d699bc9d105.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/b95274074669051e0463e4d301553923.jpg">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/134055b5ca2371efb7a9670c35cd1738.jpg">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/750979b77da92a05da1719130d597541.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/6f42de1db151e72bca1f4c5d3ca9593e.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/ed19c4f0849119e9a0fd36d9767fb125.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/b0088079618af26a5c0769e10e6756f3.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/2bcb1d5c728f9008f8c0f559678ed0d5.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/a13c7bbb5f152678cd709aa4747ce4bd.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/3f00c9806e9036cf1217f87afeae9e74.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/40e2898064d2688becd61971136b245c.png">
<meta property="og:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/36ad246bf8b5382487fb02bbd5ba2974.png">
<meta property="og:updated_time" content="2018-05-24T14:51:08.850Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="seq2seq与Attention机制">
<meta name="twitter:description" content="seq2seq与Attention机制">
<meta name="twitter:image" content="http://yoursite.com/2018/05/24/seq2seq与Attention机制/3c26dc6175b4e85196d0204a0ff0a339.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/05/24/seq2seq与Attention机制/"/>





  <title>seq2seq与Attention机制 | JinZeng's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JinZeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">nlp</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/24/seq2seq与Attention机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JinZeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JinZeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">seq2seq与Attention机制</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-24T22:48:22+08:00">
                2018-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
              <div class="post-description">
                  seq2seq与Attention机制
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h1><p>本章问题：</p>
<ol>
<li><p>seq2seq解码器端输出层需要一层projection layer层，映射到vocab的维度</p>
</li>
<li><p>seq2seq的dynamic_decode的输出outputs.rnn_output维度必须搞清楚。</p>
</li>
<li><p>seq2seq的Helper对象作用</p>
</li>
<li><p>Attention如何用tensorflow实现</p>
</li>
<li><p>如何实现multiAttention</p>
</li>
<li><p>分层attention</p>
</li>
</ol>
<h2 id="我们所用的seq2seq模型"><a href="#我们所用的seq2seq模型" class="headerlink" title="我们所用的seq2seq模型"></a>我们所用的seq2seq模型</h2><p><img src="/2018/05/24/seq2seq与Attention机制/3c26dc6175b4e85196d0204a0ff0a339.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/1def1af24886828c9e28cdd0f491f94e.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/8015d70afd6fd139d698cfcce1e204f2.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/08b30a3eed06f21457d32031fe66649e.png" alt=""></p>
<p><strong>dynamic_rnn怎么处理变长的句子：会通过参数source_sequence_length告诉dynamic_rnn输入的一批数据的长度，dynamic_rnn该函数在超过序列长度的time_step输出为0，状态保持上一个时间步的状态。</strong></p>
<p><strong>具体用法如下所示：</strong></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/454fac3258597c811778954f9f2d2ab2.png" alt=""></p>
<p><strong>我们再定义解码器的时候需要定义一个helper的对象，再用decoder_cell和helper定义decoder，运行decoder即可用dynamic_decode(decoder……)。dynamic_decode返回的参数(final_outputs,<br>final_state, final_sequence_lengths)，也就是最后时间步的输出和状态。</strong></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/55f3494c4127aa08148dcb273063cc26.png" alt=""></p>
<p><strong>Logits维度[max_decoder_time, batch_size, vocab_size]</strong></p>
<p><strong>对于TrainingHelper函数，定义了解码器的输出logits采用的获取id的采样方法：</strong></p>
<p><strong>第一个参数是潜入后的输入， time_major=True时，inputs的shape为[sequence_length,<br>batch_size, embedding_size] </strong></p>
<p><strong>sequence_length：指的是当前batch中每个序列的长度(self._batch_size =<br>array_ops.size(sequence_length))。 </strong></p>
<p>Helper.py文件</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/a4958aeeb7ae5b0a277111935b161be0.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/bcbfe42587a078103e8aaf9c9d7bd870.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/3deca48973c2383bfe7521c62dda3484.png" alt=""></p>
<p>具体是如何计算的：将解码器的标准输出标签decoder_outputs[max_decoder_time,<br>batch_size]与解码器预测的<strong>logits[max_decoder_time, batch_size,<br>vocab_size]</strong>做交叉熵，<strong>至于这里为什么要除以batch_size其实我也不是很懂。</strong></p>
<p>需要指出的是我们用 loss 除了个 batch_size，所以我们的超参数对 batch_size<br>来讲是“不变的”（不相关的）。有些人用 loss 除以 （batch_size *<br>num_time_steps），这样做会减少在短句子上产生的错误。</p>
<h2 id="Seq2seq翻译推理"><a href="#Seq2seq翻译推理" class="headerlink" title="Seq2seq翻译推理"></a>Seq2seq翻译推理</h2><p><img src="/2018/05/24/seq2seq与Attention机制/5aea7c937fc04da09b56311646f75015.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/66e9e3b23a0239690eeda744a8cf5147.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/5fb8b74dc601441211fa0ea4a72090f6.jpg" alt="preview"></p>
<p><strong>推理与训练的区别在于步骤<br>3。推理不总是馈送作为输入的正确目标词，而是使用被模型预测的单词，因为我们推理的时候没办法事先知道正确解码的句子。</strong></p>
<h2 id="分层注意力机制"><a href="#分层注意力机制" class="headerlink" title="分层注意力机制"></a>分层注意力机制</h2><p><img src="/2018/05/24/seq2seq与Attention机制/5c3653e5adc717a4d74bcc6903535a15.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/1e700a0d1d359df36e69b1c292da0c6f.png" alt=""></p>
<h2 id="点乘注意力"><a href="#点乘注意力" class="headerlink" title="点乘注意力"></a>点乘注意力</h2><p><img src="/2018/05/24/seq2seq与Attention机制/75450dcdbdbbb2bbb966f5e3feb74551.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/cb72243a207c961e4a0e63fcca414627.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/bb38fac0cacf1432540bd1854d15adb2.png" alt=""></p>
<p><strong>重要的事情说三遍：</strong></p>
<p><strong>我们attention的对象是谁，就要对该对象进行加权求和。假如我们attention的是编码器各个时间步的状态，就要对各个时间步的状态向量加权求和。</strong></p>
<p><strong>我们attention的对象是谁，就要对该对象进行加权求和。假如我们attention的是编码器各个时间步的状态，就要对各个时间步的状态向量加权求和。</strong></p>
<p><strong>我们attention的对象是谁，就要对该对象进行加权求和。假如我们attention的是编码器各个时间步的状态，就要对各个时间步的状态向量加权求和。</strong></p>
<p>这里我们简单解释一下，这里我们把矩阵V想象成一个列向量比较好，每个元素表示一个状态向量：</p>
<p>QKT表示这样的一个矩阵（n*m）：第一行是query的第一个time_step状态向量（也就是第一个词）与编码器所有time_step的状态计算出的权重向量（编码器有多长，权重就有多少个），用该权重向量乘以V向量就得到了解码器第一个time_step的attention向量。</p>
<p>那如果是selfAttetion呢？Q、K、V可能同时是编码器的状态矩阵或者解码器的状态矩阵。那么QKT矩阵的第一行是编码器第一个时间步与所有时间步的相似度权重向量，该权重向量和V向量加权求和得到新的向量，其他同理。最后我们把n*d_k的序列Q编码成新的n*d_k的序列。</p>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p>Multi-head Attention<br>允许模型联合关注不同位置的不同表征子空间信息，我们可以理解为在参数不共享的情况下，多次执行点乘注意力</p>
<p>在原论文和实现中，研究者使用了 h=8 个并行点乘注意力层而完成 Multi-head<br>Attention。对于每一个注意力层，原论文使用的维度是<br>d_k=d_v=d_model/h=64。由于每一个并行注意力层的维度降低，总的计算成本和单个点乘注意力在全维度上的成本非常相近。</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/18eebbbb56bc96ac5a50f74579b598a1.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/6ee2e9675718c6fb50dac37231871d8f.png" alt=""></p>
<p>它其实就是多个点乘注意力并行地处理并最后将结果拼接在一起。一般而言，我们可以对三个输入矩阵<br>Q、V、K 分别进行 h 个不同的线性变换，然后分别将它们投入 h<br>个点乘注意力函数并拼接所有的输出结果。</p>
<p>首先我们会取 query 的第一个维度作为批量样本数，然后再实现多个线性变换将 d_model<br>维的词嵌入向量压缩到 d_k<br>维的隐藏向量，变换后的矩阵将作为点乘注意力的输入。点乘注意力输出的矩阵将在最后一个维度拼接，即<br>8 个 n×64 维的矩阵拼接为 n×512 维的大矩阵，其中 n<br>为批量数。这样我们就将输出向量恢复为与词嵌入向量相等的维度。</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/a5751311d8086511c1a5d2da996f79da.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/710418ff7e7a7abdb1cbc6ef95ce9178.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/1b2ec6741b499ce8bb46097e0300f5fa.png" alt=""></p>
<p><strong>如何实现多头的attention：</strong></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/90bf091dbdc36e3b08d92e4de3dea8b7.png" alt=""></p>
<h2 id="位置Embedding"><a href="#位置Embedding" class="headerlink" title="位置Embedding"></a>位置Embedding</h2><p><img src="/2018/05/24/seq2seq与Attention机制/6a3345421be62897b556458a353c3d25.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/0284f856a5de915ad115471293704447.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/000bb0b82a1cfa56c558e93cf826c347.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/0744aaf83d952aaab55d47d721cd6367.png" alt=""></p>
<p>一般的，这种纯selfAttention机制训练一个文本分类或者机器翻译模型，效果应该都还不错，但是用来训练一个序列标注模型（分词、实体识别等），效果就不怎么好了。原因是机器翻译并不特别强调语序，因此PositionEmbedding所带来的位置信息以及足够了，此外翻译任务的评测指标BLUE也并不特别强调语序。</p>
<h2 id="实例分析："><a href="#实例分析：" class="headerlink" title="实例分析："></a>实例分析：</h2><p><img src="/2018/05/24/seq2seq与Attention机制/1e3f37d7e85992cb12ce0b40cdcb5fbc.png" alt="https://img-blog.csdn.net/20170722184157776?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3V6cUNob20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/bce67264ddda775bef14e6f3154e7a99.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/a2bfd2b8314d5eda5150e70649973e96.png" alt=""></p>
<p>参数学习：</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/b284b79814abd3dd08f460e9266eeaf3.png" alt=""></p>
<p>注意力机制的一个很好副产品是源语句和目标语句之间的一个易于可视化的对齐矩阵</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/510dda99021713b52c1604621f1d213c.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/191a3008d1a58ea913c19d699bc9d105.png" alt=""></p>
<h2 id="Attention中涉及的矩阵运算"><a href="#Attention中涉及的矩阵运算" class="headerlink" title="Attention中涉及的矩阵运算"></a>Attention中涉及的矩阵运算</h2><p>当前目标隐蔽状态和所有源状态（source<br>state）进行比较，以导出权重（weight）。基于注意力权重，我们计算了一个背景向量（context<br>vector），作为源状态的平均权值。<br>将背景向量与当前目标隐蔽态进行结合以生成最终的注意力向量。</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/b95274074669051e0463e4d301553923.jpg" alt="preview"></p>
<p>注意力向量是怎么生成的？生成的上下文向量（编码器隐状态的加权求和）与当前步解码器的状态拼接输入一个tanh激活函数的全连接神经网，最后再通过softmax分类即可。</p>
<p>Score函数也可以用以下方法：</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/134055b5ca2371efb7a9670c35cd1738.jpg" alt="preview"></p>
<p><strong>第一种是两个向量简单相乘（一个行向量乘以一个列向量是一个数，也可以理解为相似度），另一种是通过一层神经网络实现</strong></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/750979b77da92a05da1719130d597541.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/6f42de1db151e72bca1f4c5d3ca9593e.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/ed19c4f0849119e9a0fd36d9767fb125.png" alt=""></p>
<p>这里的memory就是我们的attention_states</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/b0088079618af26a5c0769e10e6756f3.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/2bcb1d5c728f9008f8c0f559678ed0d5.png" alt=""></p>
<p>首先要有一个query向量，和一段key向量，这里query可以理解为一个包含比较多信息的全局向量，这段key向量就是我们想要attention的对象，attention操作就是利用这个query对所有key向量进行加权求和。</p>
<h2 id="谷歌Transformer的整体框架"><a href="#谷歌Transformer的整体框架" class="headerlink" title="谷歌Transformer的整体框架"></a>谷歌Transformer的整体框架</h2><p>Transformer<br>的整体架构也采用了这种编码器-解码器的框架，它使用了多层自注意力机制和层级归一化，编码器和解码器都会使用全连接层和残差连接。Transformer<br>的整体结构如下图所示：</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/a13c7bbb5f152678cd709aa4747ce4bd.png" alt=""></p>
<p><img src="/2018/05/24/seq2seq与Attention机制/3f00c9806e9036cf1217f87afeae9e74.png" alt=""></p>
<p>编码器由相同的 6<br>个模块堆叠而成，每一个模块都有两个子层级构成。其中第一个子层级是 Multi-Head<br>自注意机制，其中自注意力表示输入和输出序列都是同一条。第二个子层级采用了全连接网络，主要作用在于注意子层级的特征。此外，每一个子层级都会添加一个残差连接和层级归一化。</p>
<p><img src="/2018/05/24/seq2seq与Attention机制/40e2898064d2688becd61971136b245c.png" alt=""></p>
<h2 id="附加："><a href="#附加：" class="headerlink" title="附加："></a>附加：</h2><p><img src="/2018/05/24/seq2seq与Attention机制/36ad246bf8b5382487fb02bbd5ba2974.png" alt=""></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/22/leetcode高级算法/" rel="next" title="leetcode高级算法">
                <i class="fa fa-chevron-left"></i> leetcode高级算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/25/LSTM变长文本处理/" rel="prev" title="循环神经网络变长文本处理">
                循环神经网络变长文本处理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JinZeng</p>
              <p class="site-description motion-element" itemprop="description">nlper</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/jin-ge-ge-21-50-31/activities" target="_blank" title="zhihu">
                      
                        <i class="fa fa-fw fa-globe"></i>zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention机制"><span class="nav-number">1.</span> <span class="nav-text">Attention机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#我们所用的seq2seq模型"><span class="nav-number">1.1.</span> <span class="nav-text">我们所用的seq2seq模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2seq翻译推理"><span class="nav-number">1.2.</span> <span class="nav-text">Seq2seq翻译推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分层注意力机制"><span class="nav-number">1.3.</span> <span class="nav-text">分层注意力机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#点乘注意力"><span class="nav-number">1.4.</span> <span class="nav-text">点乘注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多头注意力"><span class="nav-number">1.5.</span> <span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#位置Embedding"><span class="nav-number">1.6.</span> <span class="nav-text">位置Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实例分析："><span class="nav-number">1.7.</span> <span class="nav-text">实例分析：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention中涉及的矩阵运算"><span class="nav-number">1.8.</span> <span class="nav-text">Attention中涉及的矩阵运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#谷歌Transformer的整体框架"><span class="nav-number">1.9.</span> <span class="nav-text">谷歌Transformer的整体框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#附加："><span class="nav-number">1.10.</span> <span class="nav-text">附加：</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JinZeng</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
